{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Text Analysis\n",
    "\n",
    "- **Instructor:** Li Zeng ([lizeng@uw.edu](mailto:lizeng@uw.edu))\n",
    "- **Date:** Nov 1, 2018\n",
    "- **Course:** IMT 547 AU18 - Social Media Data Mining and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to perform text analysis using [NLTK](https://www.nltk.org/). NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. It is free, opensource, easy to use, large community, and well documented.\n",
    "\n",
    "### Topics\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Text Classification using Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intall and import NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In your terminal:\n",
    "#pip install nltk\n",
    "\n",
    "#Loading NLTK\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sakura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK corpora\n",
    "#nltk.download()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data - threads on [Topix](http://www.topix.com/forum/us)\n",
    "\n",
    "In this tutorial, we will work on a sample of text data scraped from a public forum on Topix for the local Santa Cruz community to discuss news and issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading padnas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"topix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4580 entries, 0 to 4579\n",
      "Data columns (total 15 columns):\n",
      "thread_id          4580 non-null int64\n",
      "title              4580 non-null object\n",
      "author_name        4580 non-null object\n",
      "author_geo         4580 non-null object\n",
      "registered_user    4580 non-null bool\n",
      "comment_order      4580 non-null object\n",
      "comment_date       4580 non-null object\n",
      "post_content       4580 non-null object\n",
      "judge1_title       4580 non-null object\n",
      "judge1_count       4580 non-null int64\n",
      "judge2_title       4231 non-null object\n",
      "judge2_count       4580 non-null int64\n",
      "judge3_title       3972 non-null object\n",
      "judge3_count       4580 non-null int64\n",
      "score              4580 non-null object\n",
      "dtypes: bool(1), int64(4), object(10)\n",
      "memory usage: 505.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_geo</th>\n",
       "      <th>registered_user</th>\n",
       "      <th>comment_order</th>\n",
       "      <th>comment_date</th>\n",
       "      <th>post_content</th>\n",
       "      <th>judge1_title</th>\n",
       "      <th>judge1_count</th>\n",
       "      <th>judge2_title</th>\n",
       "      <th>judge2_count</th>\n",
       "      <th>judge3_title</th>\n",
       "      <th>judge3_count</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Norse distributes flier promoting executing ho...</td>\n",
       "      <td>Give Up Santa Cruz</td>\n",
       "      <td>Santa Cruz, CA</td>\n",
       "      <td>False</td>\n",
       "      <td>#1</td>\n",
       "      <td>25-Oct-13</td>\n",
       "      <td>https://www.indybay.org/newsitems/2013/10/23/....</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>1</td>\n",
       "      <td>Incendiary</td>\n",
       "      <td>1</td>\n",
       "      <td>Disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Norse distributes flier promoting executing ho...</td>\n",
       "      <td>Bud</td>\n",
       "      <td>Santa Cruz, CA</td>\n",
       "      <td>False</td>\n",
       "      <td>#2</td>\n",
       "      <td>25-Oct-13</td>\n",
       "      <td>Give Up Santa Cruz wrote:   https://www.indy...</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>3</td>\n",
       "      <td>Agree</td>\n",
       "      <td>3</td>\n",
       "      <td>Helpful</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Norse distributes flier promoting executing ho...</td>\n",
       "      <td>DBS</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>False</td>\n",
       "      <td>#3</td>\n",
       "      <td>25-Oct-13</td>\n",
       "      <td>It's really too bad that Robert doesn't have h...</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>4</td>\n",
       "      <td>Agree</td>\n",
       "      <td>4</td>\n",
       "      <td>Helpful</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Norse distributes flier promoting executing ho...</td>\n",
       "      <td>jeff helms</td>\n",
       "      <td>Walnut Creek, CA</td>\n",
       "      <td>False</td>\n",
       "      <td>#4</td>\n",
       "      <td>25-Oct-13</td>\n",
       "      <td>Give Up Santa Cruz wrote:   https://www.indy...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>4</td>\n",
       "      <td>Clueless</td>\n",
       "      <td>4</td>\n",
       "      <td>Nuts</td>\n",
       "      <td>4</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Norse distributes flier promoting executing ho...</td>\n",
       "      <td>jeff helms</td>\n",
       "      <td>Walnut Creek, CA</td>\n",
       "      <td>False</td>\n",
       "      <td>#5</td>\n",
       "      <td>25-Oct-13</td>\n",
       "      <td>Give Up Santa Cruz wrote:   https://www.indy...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>3</td>\n",
       "      <td>Clueless</td>\n",
       "      <td>3</td>\n",
       "      <td>Nuts</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   thread_id                                              title  \\\n",
       "0          1  Norse distributes flier promoting executing ho...   \n",
       "1          1  Norse distributes flier promoting executing ho...   \n",
       "2          1  Norse distributes flier promoting executing ho...   \n",
       "3          1  Norse distributes flier promoting executing ho...   \n",
       "4          1  Norse distributes flier promoting executing ho...   \n",
       "\n",
       "          author_name        author_geo  registered_user comment_order  \\\n",
       "0  Give Up Santa Cruz    Santa Cruz, CA            False            #1   \n",
       "1                 Bud    Santa Cruz, CA            False            #2   \n",
       "2                 DBS     Cupertino, CA            False            #3   \n",
       "3          jeff helms  Walnut Creek, CA            False            #4   \n",
       "4          jeff helms  Walnut Creek, CA            False            #5   \n",
       "\n",
       "  comment_date                                       post_content  \\\n",
       "0    25-Oct-13  https://www.indybay.org/newsitems/2013/10/23/....   \n",
       "1    25-Oct-13    Give Up Santa Cruz wrote:   https://www.indy...   \n",
       "2    25-Oct-13  It's really too bad that Robert doesn't have h...   \n",
       "3    25-Oct-13    Give Up Santa Cruz wrote:   https://www.indy...   \n",
       "4    25-Oct-13    Give Up Santa Cruz wrote:   https://www.indy...   \n",
       "\n",
       "  judge1_title  judge1_count judge2_title  judge2_count judge3_title  \\\n",
       "0  Interesting             1   Incendiary             1     Disagree   \n",
       "1    Brilliant             3        Agree             3      Helpful   \n",
       "2    Brilliant             4        Agree             4      Helpful   \n",
       "3         Spam             4     Clueless             4         Nuts   \n",
       "4         Spam             3     Clueless             3         Nuts   \n",
       "\n",
       "   judge3_count     score  \n",
       "0             1  Negative  \n",
       "1             3  Positive  \n",
       "2             4  Positive  \n",
       "3             4  Negative  \n",
       "4             3  Negative  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    545\n",
       "Positive    455\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "Before we analyze and mine text data, we want it to contain as less noise as possible. However, it is not usully the case for texts collected from our real world. Therefore, we need to de-noise our raw text.\n",
    "\n",
    "#### Sample noise removal tasks could include:\n",
    "* removing text file headers, footers\n",
    "* removing HTML, XML, etc. markup and metadata\n",
    "* extracting valuable data from other formats, such as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.indybay.org/newsitems/2013/10/23/....\n",
       "1      Give Up Santa Cruz wrote:   https://www.indy...\n",
       "2    It's really too bad that Robert doesn't have h...\n",
       "3      Give Up Santa Cruz wrote:   https://www.indy...\n",
       "4      Give Up Santa Cruz wrote:   https://www.indy...\n",
       "Name: post_content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw text\n",
    "data['post_content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_cleaning(x):    \n",
    "    # we want to remove urls\n",
    "    cleaned_x = re.sub(r'http\\S+','',x)\n",
    "    # we want to remove non-ASCII characters from list of tokenized words\n",
    "    #cleaned_x = cleaned_x.decode('utf-8', 'ignore')\n",
    "    # you can implement other cleaning task here\n",
    "    return cleaned_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['post_content'] = data['post_content'].apply(customized_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 His words, not mine.\n",
       "1      Give Up Santa Cruz wrote:    words, not mine...\n",
       "2    It's really too bad that Robert doesn't have h...\n",
       "3      Give Up Santa Cruz wrote:    words, not mine...\n",
       "4      Give Up Santa Cruz wrote:    words, not mine...\n",
       "Name: post_content, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['post_content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is building blocks for sentence or paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' His words, not mine.'\n",
      " '  Give Up Santa Cruz wrote:    words, not mine.   Rather than click a link and give Indybay more visitors just let it be what it is: A theatrical display of ignorance.'\n",
      " \"It's really too bad that Robert doesn't have his pathetic fliers printed on toilet paper.Then for the first time, he would have done something to help the homeless he pretends to care so much about!!\"\n",
      " '  Give Up Santa Cruz wrote:    words, not mine.   Don\\'t you think its kinda of ironic people think they can attack and shove aside someone who is poor?--most egged on by the prostitution racket?? Where would these prostitutes be without prostitution?? HOMELESS!--there\\'s the \"get the tweakers\" crowd also--and \"tweakers\" are people also and might turn into a fascist movement etc.--we all know what a horrible monster fascism is. The anti zoonotic crowd has legitimate complaint along with private property owners. The anti-drug crowd needs to look at the std/drug epidemic and what\\'s causing it--the foreign organized drug fronts and who set up and supported foreign organized drug fronts: The \"wash narcotic money in real estate\" rackets who pretty much made the \"homelessness\" and \"crime\" and std rates that are running our society in the ground with HUGE std/drug healthcare bills.'\n",
      " '  Give Up Santa Cruz wrote:    words, not mine.   Robert is a solid american with solid american values and standards- beckey is twisted criminal pretty much--I think your confusing robert with beckey.']\n"
     ]
    }
   ],
   "source": [
    "# let's first play around the first three posts in this sample\n",
    "posts = data['post_content']\n",
    "print(posts.head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([' His words, not mine.'])\n",
      " list(['  Give Up Santa Cruz wrote:    words, not mine.', 'Rather than click a link and give Indybay more visitors just let it be what it is: A theatrical display of ignorance.'])\n",
      " list([\"It's really too bad that Robert doesn't have his pathetic fliers printed on toilet paper.Then for the first time, he would have done something to help the homeless he pretends to care so much about!\", '!'])\n",
      " list(['  Give Up Santa Cruz wrote:    words, not mine.', \"Don't you think its kinda of ironic people think they can attack and shove aside someone who is poor?--most egged on by the prostitution racket??\", 'Where would these prostitutes be without prostitution??', 'HOMELESS!--there\\'s the \"get the tweakers\" crowd also--and \"tweakers\" are people also and might turn into a fascist movement etc.--we all know what a horrible monster fascism is.', 'The anti zoonotic crowd has legitimate complaint along with private property owners.', 'The anti-drug crowd needs to look at the std/drug epidemic and what\\'s causing it--the foreign organized drug fronts and who set up and supported foreign organized drug fronts: The \"wash narcotic money in real estate\" rackets who pretty much made the \"homelessness\" and \"crime\" and std rates that are running our society in the ground with HUGE std/drug healthcare bills.'])\n",
      " list(['  Give Up Santa Cruz wrote:    words, not mine.', 'Robert is a solid american with solid american values and standards- beckey is twisted criminal pretty much--I think your confusing robert with beckey.'])]\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization - breaks text paragraph into sentences.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text = posts.apply(sent_tokenize)\n",
    "print(tokenized_text.head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['His', 'words', ',', 'not', 'mine', '.'])\n",
      " list(['Give', 'Up', 'Santa', 'Cruz', 'wrote', ':', 'words', ',', 'not', 'mine', '.', 'Rather', 'than', 'click', 'a', 'link', 'and', 'give', 'Indybay', 'more', 'visitors', 'just', 'let', 'it', 'be', 'what', 'it', 'is', ':', 'A', 'theatrical', 'display', 'of', 'ignorance', '.'])\n",
      " list(['It', \"'s\", 'really', 'too', 'bad', 'that', 'Robert', 'does', \"n't\", 'have', 'his', 'pathetic', 'fliers', 'printed', 'on', 'toilet', 'paper.Then', 'for', 'the', 'first', 'time', ',', 'he', 'would', 'have', 'done', 'something', 'to', 'help', 'the', 'homeless', 'he', 'pretends', 'to', 'care', 'so', 'much', 'about', '!', '!'])\n",
      " list(['Give', 'Up', 'Santa', 'Cruz', 'wrote', ':', 'words', ',', 'not', 'mine', '.', 'Do', \"n't\", 'you', 'think', 'its', 'kinda', 'of', 'ironic', 'people', 'think', 'they', 'can', 'attack', 'and', 'shove', 'aside', 'someone', 'who', 'is', 'poor', '?', '--', 'most', 'egged', 'on', 'by', 'the', 'prostitution', 'racket', '?', '?', 'Where', 'would', 'these', 'prostitutes', 'be', 'without', 'prostitution', '?', '?', 'HOMELESS', '!', '--', 'there', \"'s\", 'the', '``', 'get', 'the', 'tweakers', \"''\", 'crowd', 'also', '--', 'and', '``', 'tweakers', \"''\", 'are', 'people', 'also', 'and', 'might', 'turn', 'into', 'a', 'fascist', 'movement', 'etc.', '--', 'we', 'all', 'know', 'what', 'a', 'horrible', 'monster', 'fascism', 'is', '.', 'The', 'anti', 'zoonotic', 'crowd', 'has', 'legitimate', 'complaint', 'along', 'with', 'private', 'property', 'owners', '.', 'The', 'anti-drug', 'crowd', 'needs', 'to', 'look', 'at', 'the', 'std/drug', 'epidemic', 'and', 'what', \"'s\", 'causing', 'it', '--', 'the', 'foreign', 'organized', 'drug', 'fronts', 'and', 'who', 'set', 'up', 'and', 'supported', 'foreign', 'organized', 'drug', 'fronts', ':', 'The', '``', 'wash', 'narcotic', 'money', 'in', 'real', 'estate', \"''\", 'rackets', 'who', 'pretty', 'much', 'made', 'the', '``', 'homelessness', \"''\", 'and', '``', 'crime', \"''\", 'and', 'std', 'rates', 'that', 'are', 'running', 'our', 'society', 'in', 'the', 'ground', 'with', 'HUGE', 'std/drug', 'healthcare', 'bills', '.'])\n",
      " list(['Give', 'Up', 'Santa', 'Cruz', 'wrote', ':', 'words', ',', 'not', 'mine', '.', 'Robert', 'is', 'a', 'solid', 'american', 'with', 'solid', 'american', 'values', 'and', 'standards-', 'beckey', 'is', 'twisted', 'criminal', 'pretty', 'much', '--', 'I', 'think', 'your', 'confusing', 'robert', 'with', 'beckey', '.'])]\n"
     ]
    }
   ],
   "source": [
    "# word tokenization - breaks text paragraph into words.\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=posts.apply(word_tokenize)\n",
    "print(tokenized_word.head().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.\n",
    "\n",
    "Remember, after tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                        [words, mine]\n",
      "1    [give, santa, cruz, wrote, words, mine, rather...\n",
      "2    [really, bad, robert, nt, pathetic, fliers, pr...\n",
      "3    [give, santa, cruz, wrote, words, mine, nt, th...\n",
      "4    [give, santa, cruz, wrote, words, mine, robert...\n",
      "Name: post_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "normalized_posts = tokenized_word.apply(normalize)\n",
    "print(normalized_posts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        [words, mine]\n",
       "1    [give, santa, cruz, wrote, words, mine, rather...\n",
       "2    [really, bad, robert, nt, pathetic, fliers, pr...\n",
       "3    [give, santa, cruz, wrote, words, mine, nt, th...\n",
       "4    [give, santa, cruz, wrote, words, mine, robert...\n",
       "5    [people, using, homeless, racketeer, property,...\n",
       "6    [looks, like, roberts, flyer, coming, true, le...\n",
       "7    [punk, ass, wannabes, think, push, americans, ...\n",
       "8    [expensive, apartment, losing, house, illegall...\n",
       "9    [bud, wrote, quoted, text, rather, click, link...\n",
       "Name: post_content, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_posts.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          [word, min]\n",
       "1    [giv, sant, cruz, wrot, word, min, rath, click...\n",
       "2    [real, bad, robert, nt, pathet, fli, print, to...\n",
       "3    [giv, sant, cruz, wrot, word, min, nt, think, ...\n",
       "4    [giv, sant, cruz, wrot, word, min, robert, sol...\n",
       "5    [peopl, us, homeless, racket, property, etc, r...\n",
       "6    [look, lik, robert, fly, com, tru, leigh, say,...\n",
       "7    [punk, ass, wannab, think, push, am, around, d...\n",
       "8    [expend, apart, los, hous, illeg, racket, prop...\n",
       "9    [bud, wrot, quot, text, rath, click, link, giv...\n",
       "Name: post_content, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_posts.iloc[:10].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                         [word, mine]\n",
       "1    [give, santa, cruz, write, word, mine, rather,...\n",
       "2    [really, bad, robert, nt, pathetic, fliers, pr...\n",
       "3    [give, santa, cruz, write, word, mine, nt, thi...\n",
       "4    [give, santa, cruz, write, word, mine, robert,...\n",
       "5    [people, use, homeless, racketeer, property, e...\n",
       "6    [look, like, roberts, flyer, come, true, leigh...\n",
       "7    [punk, ass, wannabes, think, push, americans, ...\n",
       "8    [expensive, apartment, lose, house, illegally,...\n",
       "9    [bud, write, quote, text, rather, click, link,...\n",
       "Name: post_content, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_posts.iloc[:10].apply(lemmatize_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the stemmed and lemmatized results. Depending on your upcoming NLP task or preference, one of these may be more appropriate than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the text for ML\n",
    "text = normalized_posts.apply(lemmatize_verbs)\n",
    "text_str = [\" \".join(t) for t in text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(text_str)\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "vectorizer.fit(x)\n",
    "X = vectorizer.transform(x)\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, data['score'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 5633) (700,)\n",
      "(700, 5633) (300,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape,  Y_train.shape)\n",
    "print(X_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# call the fitted model fitted_model, for future reference:\n",
    "fitted_model = MultinomialNB()\n",
    "fitted_model.fit(X_train, Y_train)\n",
    "print(fitted_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_tf, data['score'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6466666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# call the fitted model fitted_model, for future reference:\n",
    "fitted_model = MultinomialNB()\n",
    "fitted_model.fit(X_train, Y_train)\n",
    "print(fitted_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
