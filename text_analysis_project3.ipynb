{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sakura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #download the latest stopwords\n",
    "\n",
    "#stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sakura\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (3,4,6,9,13,14,15,16,17,24,25,28,29,34,37,38,39,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Sakura\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (3,4,6,9,13,14,15,16,17,24,25,28,29,32,34,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Sakura\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (3,4,6,9,13,14,15,16,17,24,25,28,29,34,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(815897,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading only data1 due to memory error \n",
    "\n",
    "df_jobs_data1 = pd.read_csv('./bu_reviews_materialized1.csv',error_bad_lines=False)\n",
    "df_jobs_data2 = pd.read_csv('./bu_reviews_materialized2.csv',error_bad_lines=False)\n",
    "df_jobs_data3 = pd.read_csv('./bu_reviews_materialized3.csv',error_bad_lines=False)\n",
    "\n",
    "df_jobs_data1['b_text1'] = df_jobs_data1['b_text'].astype('U').values\n",
    "df_jobs_data2['b_text1'] = df_jobs_data2['b_text'].astype('U').values\n",
    "df_jobs_data= df_jobs_data1.append(df_jobs_data2, ignore_index=True)\n",
    "del df_jobs_data1\n",
    "del df_jobs_data2\n",
    "df_jobs_data3['b_text1'] = df_jobs_data3['b_text'].astype('U').values\n",
    "\n",
    "df_jobs_data= df_jobs_data.append(df_jobs_data3, ignore_index=True)\n",
    "#df_jobs_data['b_text1'] = df_jobs_data['b_text'].values.astype('U')\n",
    "del df_jobs_data3\n",
    "df_jobs_data.b_text.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(corpus,\n",
    "                       NGRAM_RANGE=(0,1),\n",
    "                       stop_words = None,\n",
    "                        stem = False,\n",
    "                       MIN_DF = 0.05,\n",
    "                       MAX_DF = 0.95,\n",
    "                       USE_IDF=False):\n",
    "       #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "     \n",
    "    if stem:\n",
    "        tokenize = lambda x: [stemmer.stem(i) for i in x.split()]\n",
    "    else:\n",
    "        tokenize = None\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                tokenizer=tokenize, \n",
    "                                ngram_range=NGRAM_RANGE,\n",
    "                                stop_words = stop_words,\n",
    "                                strip_accents=STRIP_ACCENTS,\n",
    "                                min_df = MIN_DF,\n",
    "                                max_df = MAX_DF)\n",
    "    \n",
    "    bag_of_words = vectorizer.fit_transform( corpus ) #transform our corpus is a bag of words \n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    if USE_IDF:\n",
    "        NORM = None #turn on normalization flag\n",
    "        SMOOTH_IDF = True #prvents division by zero errors\n",
    "        SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "        transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "        #get the bag-of-words from the vectorizer and\n",
    "        #then use TFIDF to limit the tokens found throughout the text \n",
    "        tfidf = transformer.fit_transform(bag_of_words)\n",
    "        \n",
    "        return tfidf, features\n",
    "    else:\n",
    "        return bag_of_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(bag_of_words, feature_names):\n",
    "    np_bag_of_words = bag_of_words.toarray()\n",
    "    word_count = np.sum(np_bag_of_words,axis=0)\n",
    "    np_word_count = np.asarray(word_count).ravel()\n",
    "    dict_word_counts = dict( zip(feature_names, np_word_count) )\n",
    "    \n",
    "    orddict_word_counts = OrderedDict( \n",
    "                                    sorted(dict_word_counts.items(), key=lambda x: x[1], reverse=True), )\n",
    "    \n",
    "    return orddict_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_jobs_data['b_text1'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics(tfidf, features, N_TOPICS=3, N_TOP_WORDS=5,):\n",
    "    with progressbar.ProgressBar(max_value=progressbar.UnknownLength) as bar:\n",
    "        i=0\n",
    "        lda = LatentDirichletAllocation( n_topics= N_TOPICS,\n",
    "                                       learning_method='online') #create an object that will create 5 topics\n",
    "        bar.update(i)\n",
    "        i+=1\n",
    "        doctopic = lda.fit_transform( tfidf )\n",
    "        bar.update(i)\n",
    "        i+=1\n",
    "        \n",
    "        ls_keywords = []\n",
    "        for i,topic in enumerate(lda.components_):\n",
    "            word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "            keywords = ', '.join( features[i] for i in word_idx)\n",
    "            ls_keywords.append(keywords)\n",
    "            print(i, keywords)\n",
    "            bar.update(i)\n",
    "            i+=1\n",
    "            \n",
    "    return ls_keywords, doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n",
    "RE_PREPROCESS = r'\\W+|\\d+' #the regular expressions that matches all non-characters\n",
    "\n",
    "#get rid of punctuation and make everything lowercase\n",
    "#the code belows works by looping through the array of text\n",
    "#for a given piece of text we invoke the `re.sub` command where we pass in the regular expression, a space ' ' to\n",
    "#subsitute all the matching characters with\n",
    "#we then invoke the `lower()` method on the output of the re.sub command\n",
    "#to make all the remaining characters\n",
    "#the cleaned document is then stored in a list\n",
    "#once this list has been filed it is then stored in a numpy array\n",
    "\n",
    "processed_corpus = np.array( [re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in corpus] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords =  stopwords.words('english')\n",
    "del df_jobs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ |#                                                  | 0 Elapsed Time: 0:00:00C:\\Users\\Sakura\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "| |                                              #    | 9 Elapsed Time: 1:06:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 happy, hour, favorite, steak, perfect\n",
      "1 us, table, came, said, server\n",
      "2 staff, friendly, burger, excellent, great\n",
      "3 cheese, everything, recommend, amazing, awesome\n",
      "4 bit, though, something, day, lot\n",
      "5 beer, wine, selection, location, spot\n",
      "6 sushi, chicken, fresh, salad, lunch\n",
      "7 price, give, worth, taste, overall\n",
      "8 bar, drinks, area, friends, place\n",
      "9 say, bad, take, better, thing\n"
     ]
    }
   ],
   "source": [
    "#topic model 1 without manual intervention in keywords\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=eng_stopwords,\n",
    "                                                                NGRAM_RANGE=(0,2),\n",
    "                                                                USE_IDF = True)\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['give', 'said', 'also', 'would', 'get', 'awesome', 'great', 'best', 'bad', 'good', 'us', 'excellent', 'want', 'thing', 'vegas', 'always', 'every', 'one', 'many', 'something', 'much', 'amazing', 'say', 'came', 'think', 'go', 'eat', 'make', 'even', 'super', 'though', 'took', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "remove_words1 = ['give','said','also','would','get','awesome','great','best','bad','good','us','excellent','want','thing',\n",
    "                 'vegas','always','every','one','many','something','much','amazing','say','came','think','go','eat','make','even','super','though','took','nothing']\n",
    "print(remove_words1)\n",
    "domain_specific_stopwords1 = eng_stopwords + remove_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ |#                                                  | 0 Elapsed Time: 0:00:00C:\\Users\\Sakura\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "| |#                                                  | 5 Elapsed Time: 0:58:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sushi, dinner, fresh, salad, meal, lunch\n",
      "1 beer, atmosphere, place, bar, area, worth\n",
      "2 chicken, sauce, cheese, burger, fries, steak\n",
      "3 time, table, first, order, back, got\n",
      "4 love, always, staff, friendly, place, food\n",
      "5 happy, hour, favorite, wine, perfect, location\n"
     ]
    }
   ],
   "source": [
    "#Topic Model 2\n",
    "processed_bag_of_words1, processed_features1 = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords1,\n",
    "                                                                 NGRAM_RANGE=(0,2),\n",
    "                                                                 USE_IDF = True)\n",
    "#dict_word_counts1 = get_word_counts(processed_bag_of_words1,processed_features1)\n",
    "processed_keywords1, processed_doctopic1 = create_topics(processed_bag_of_words1, \n",
    "                                                       processed_features1, \n",
    "                                                      N_TOPICS = 6,\n",
    "                                                      N_TOP_WORDS= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ |#                                                  | 0 Elapsed Time: 0:00:00C:\\Users\\Sakura\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "| |           #                                       | 5 Elapsed Time: 0:36:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time, night, people, around, minutes\n",
      "1 restaurant, server, ever, new, favorite\n",
      "2 staff, friendly, atmosphere, recommend, food\n",
      "3 sushi, fresh, lunch, fish, price\n",
      "4 love, beer, happy, hour, selection\n",
      "5 chicken, delicious, salad, sauce, cheese\n"
     ]
    }
   ],
   "source": [
    "#Topic Model 3\n",
    "processed_bag_of_words2, processed_features2 = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords1,\n",
    "                                                                 NGRAM_RANGE=(1,2),\n",
    "                                                                 USE_IDF = True)\n",
    "#dict_word_counts2 = get_word_counts(processed_bag_of_words2,processed_features2)\n",
    "processed_keywords2, processed_doctopic2 = create_topics(processed_bag_of_words2, \n",
    "                                                       processed_features2, \n",
    "                                                      N_TOPICS = 6,\n",
    "                                                      N_TOP_WORDS= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_topic_id1 = [np.argsort(processed_doctopic[comment_id])[::-1][0] for comment_id in range(len(corpus))]\n",
    "ls_topic_id2 = [np.argsort(processed_doctopic1[comment_id])[::-1][0] for comment_id in range(len(corpus))]\n",
    "ls_topic_id3 = [np.argsort(processed_doctopic2[comment_id])[::-1][0] for comment_id in range(len(corpus))]\n",
    "\n",
    "df_jobs_data['topic_id1'] = ls_topic_id1 #add to the dataframe so we can compare with the job titles\n",
    "df_jobs_data['topic_id2'] = ls_topic_id2 #add to the dataframe so we can compare with the job titles\n",
    "df_jobs_data['topic_id3'] = ls_topic_id3 #add to the dataframe so we can compare with the job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_data.to_csv(\"topic_results.csv\", sep='\\t')\n",
    "df_jobs_data.to_csv(\"topic_results1.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a_int64_field_0                                       a_categories  \\\n",
      "0              256  food tours, bars, nightlife, tours, restaurant...   \n",
      "1              256  food tours, bars, nightlife, tours, restaurant...   \n",
      "2            20480  restaurants, nightlife, pubs, american (tradit...   \n",
      "3            20480  restaurants, nightlife, pubs, american (tradit...   \n",
      "4            20480  restaurants, nightlife, pubs, american (tradit...   \n",
      "\n",
      "   a_is_open a_attributes_AgesAllowed a_attributes_HairSpecializesIn  \\\n",
      "0          1                      NaN                            NaN   \n",
      "1          1                      NaN                            NaN   \n",
      "2          1                      NaN                            NaN   \n",
      "3          1                      NaN                            NaN   \n",
      "4          1                      NaN                            NaN   \n",
      "\n",
      "  a_attributes_BusinessAcceptsBitcoin a_attributes_AcceptsInsurance  \\\n",
      "0                                 NaN                           NaN   \n",
      "1                                 NaN                           NaN   \n",
      "2                                 NaN                           NaN   \n",
      "3                                 NaN                           NaN   \n",
      "4                                 NaN                           NaN   \n",
      "\n",
      "  a_attributes_Smoking                                 a_attributes_Music  \\\n",
      "0                  NaN                                                NaN   \n",
      "1                  NaN                                                NaN   \n",
      "2              outdoor  {'dj': False, 'background_music': False, 'no_m...   \n",
      "3              outdoor  {'dj': False, 'background_music': False, 'no_m...   \n",
      "4              outdoor  {'dj': False, 'background_music': False, 'no_m...   \n",
      "\n",
      "  a_attributes_Corkage    ...    b_funny b_cool      b_date b_useful  \\\n",
      "0                  NaN    ...        5.0    9.0  2018-01-24      8.0   \n",
      "1                  NaN    ...        1.0    1.0  2018-06-25      1.0   \n",
      "2                  NaN    ...        1.0    1.0  2014-11-05      2.0   \n",
      "3                  NaN    ...        0.0    0.0  2011-07-09      2.0   \n",
      "4                  NaN    ...        0.0    0.0  2016-01-08      1.0   \n",
      "\n",
      "                b_user_id             b_review_id  \\\n",
      "0  N9hqTPQu2bmI8c2Pj8F1ww  k_WzGFaEWUszgx0OaDzV6g   \n",
      "1  kF3HKMZI8HYpkf7ZTOYD0g  SPddg_OI3tWmOuqaxQcRcw   \n",
      "2  -PeG55F9U3oMV69B2WiLNA  YlovovELQCiHa9fQyHRqSA   \n",
      "3  dWRCqHQO4Is4SFAAfVxD1g  6zJHVn4gIIxusEqWp0KSEw   \n",
      "4  WLHlDjs3HZKrqjdmW-3XkQ  A0EFxr22_OcpsddGlPi3Lw   \n",
      "\n",
      "                                             b_text1 topic_id1 topic_id2  \\\n",
      "0  There are two types of people in the world: th...         8         3   \n",
      "1  This was a fun way to spend an afternoon and e...         6         2   \n",
      "2  We decided on Paradise this last Saturday and ...         1         3   \n",
      "3  Food was good and lots of it for the price. Se...         7         3   \n",
      "4  I was pleasantly surprised with the menu! Norm...         3         2   \n",
      "\n",
      "  topic_id3  \n",
      "0         1  \n",
      "1         4  \n",
      "2         1  \n",
      "3         3  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_jobs_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
